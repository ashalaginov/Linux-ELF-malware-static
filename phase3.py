print ("Phase 3")

malwareDir="ELF/"

import mysql.connector
from mysql.connector import Error
from mysql.connector import errorcode
import os
import hashlib
import json
import subprocess
import timeit
import requests
import re

apikey='YOUR_PRIVATE VT_API'

mydb = mysql.connector.connect(host='localhost',
                                     database='ELF',
                                     user='ELF',
                                     password='toor')


# Process all entries in DB that do not have VT report and not marked with `do_not_process`='`'
mycursor = mydb.cursor()
mycursor.execute("SELECT `md5`,`virustotal_file_report`,`peframe`,`readelf`,`strings`,`size`,`file_entropy` FROM rawData WHERE `file` LIKE '%ELF%' AND `virustotal_file_report` IS NOT NULL AND (`file` LIKE '%80386%' OR `file` LIKE '%86-64%')")
myresult = mycursor.fetchall()
count=len(myresult)

MS_count=0
while count>0:

	for resource in myresult:
		query=()
		count-=1
		data = json.loads(resource[1])
		md5=resource[0]
		# Check Type and Family from Microsoft		
		if "Microsoft" in data["scans"]:
			if data["scans"]["Microsoft"]["result"] is not None:
				MS_count+=1;
				type=data["scans"]["Microsoft"]["result"].split(":")[0]
				family=data["scans"]["Microsoft"]["result"].split(":")[1].split("/")[1].split(".")[0].split("!")[0]

				# VT featueres
				vt_submission_names=len(data["submission_names"])
		        
				vt_times_submitted=data["times_submitted"]

				if "exiftool" in data["additional_info"]:
				    vt_exif=len(data["additional_info"]["exiftool"])
				else:
				    vt_exif=0

				if "embedded_ips" in data["additional_info"]:
				    vt_embedded_ips=len(data["additional_info"]["embedded_ips"])
				else:
				    vt_embedded_ips=0

				if "contacted_ips" in data["additional_info"]:
				    vt_contacted_ips=len(data["additional_info"]["contacted_ips"])
				else:
				    vt_contacted_ips=0

				if "gandelf" in data["additional_info"]:
					if "exports" in data["additional_info"]["gandelf"]:
					    vt_exports=len(data["additional_info"]["gandelf"]["exports"])
					else:
					    vt_exports=0
    			
					if "imports" in data["additional_info"]["gandelf"]:
					    vt_imports=len(data["additional_info"]["gandelf"]["imports"])
					else:
						vt_imports=0

					if "shared_libraries" in data["additional_info"]["gandelf"]:
					    vt_shared_libraries=len(data["additional_info"]["gandelf"]["shared_libraries"])
					else:
				    	    vt_shared_libraries=0

					if "segments" in data["additional_info"]["gandelf"]:
				    	    vt_segments=len(data["additional_info"]["gandelf"]["segments"])
					else:
					    vt_segments=0
		
					if "sections" in data["additional_info"]["gandelf"]:
					    vt_sections=len(data["additional_info"]["gandelf"]["sections"])
					else:
					    vt_sections=0

					if "packers" in data["additional_info"]["gandelf"]:
					    vt_packers=len(data["additional_info"]["gandelf"]["packers"])
					else:
					    vt_packers=0
				else:
					vt_exports=0
					vt_imports=0
					vt_shared_libraries=0
					vt_segments=0
					vt_sections=0
					vt_packers=0
    
				if "tags" in data:
				    vt_tags=len(data["tags"])
				else:
				    vt_tags=0

				if "positives" in data:
				    vt_positives=data["positives"]
				else:
				    vt_positives=0

				# PEframe
				if resource[2]:
					data = json.loads(resource[2])

					if "ip" in data["strings"]:
					    peframe_ip=len(data["strings"]["ip"])
					else:
					    peframe_ip=0
				
					if "url" in data["strings"]:
					    peframe_url=len(data["strings"]["url"])
					else:
					    peframe_url=0
				else:
					peframe_ip=0;
					peframe_url=0;

				# Readelf
				data =resource[3]
				readelf_entry_address=int(data.split("\n")[10].split(":")[1].strip(),16)
				readelf_start_prog_headers=re.findall("\d+",data.split("\n")[11].split(":")[1].strip())[0]
				readelf_start_sec_headers=re.findall("\d+",data.split("\n")[12].split(":")[1].strip())[0]			
				readelf_number_flags=len(data.split("\n")[13].split(":")[1].split(","))
				readelf_header_size=re.findall("\d+",data.split("\n")[14].split(":")[1].strip())[0]
				readelf_size_prog_headers=re.findall("\d+",data.split("\n")[15].split(":")[1].strip())[0]			
				readelf_number_prog_headers=re.findall("\d+",data.split("\n")[16].split(":")[1].strip())[0]
				readelf_size_sec_headers=re.findall("\d+",data.split("\n")[17].split(":")[1].strip())[0]			
				readelf_number_section_headers=re.findall("\d+",data.split("\n")[18].split(":")[1].strip())[0]
				readelf_sec_header_string_table_index=re.findall("\d+",data.split("\n")[19].split(":")[1].strip())[0]		
				

				# strings
				data=resource[4]
				strings_number=len(data.split("\n"))
				strings_size=len(data)
				strings_avg=float(len(data)/len(data.split("\n")))
				
				# file size and entropy
				file_size=resource[5]
				file_entropy=resource[6]
				
				query=(md5,type,family,vt_submission_names, vt_times_submitted, vt_exif, vt_embedded_ips, vt_contacted_ips, vt_exports, vt_imports, vt_shared_libraries, vt_segments, vt_sections, vt_packers, vt_tags,vt_positives,peframe_ip,peframe_url,readelf_entry_address, readelf_start_prog_headers, readelf_start_sec_headers, readelf_number_flags, readelf_header_size, readelf_size_prog_headers, readelf_number_prog_headers, readelf_size_sec_headers, readelf_number_section_headers, readelf_sec_header_string_table_index,strings_number,strings_size,strings_avg,file_size,file_entropy,md5)
				sql="INSERT IGNORE INTO processed (md5,type,family,vt_submission_names, vt_times_submitted, vt_exif, vt_embedded_ips, vt_contacted_ips, vt_exports, vt_imports, vt_shared_libraries, vt_segments, vt_sections, vt_packers, vt_tags,vt_positives,peframe_ip,peframe_url,readelf_entry_address, readelf_start_prog_headers, readelf_start_sec_headers, readelf_number_flags, readelf_header_size, readelf_size_prog_headers, readelf_number_prog_headers, readelf_size_sec_headers, readelf_number_section_headers, readelf_sec_header_string_table_index,strings_number,strings_size,strings_avg,file_size,file_entropy) VALUES (%s,%s,%s,%s,%s, %s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s) ON DUPLICATE KEY UPDATE md5=%s"
				mycursor.execute(sql, query)
				mydb.commit()


print("Count of MS files: "+str(MS_count))

mycursor.close()
mydb.close()
